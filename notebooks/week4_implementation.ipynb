{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 Implementation: Evolution Strategies for Non-Differentiable RL\n",
        "\n",
        "**Date:** February 6, 2026  \n",
        "**Course:** STAT 4830\n",
        "\n",
        "This notebook demonstrates a working implementation comparing Evolution Strategies (ES) with PPO on sparse reward gridworld environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Setup\n",
        "\n",
        "### Clear Problem Statement\n",
        "\n",
        "**Goal:** Learn a policy π_θ that navigates from bottom-left to top-right in a gridworld with obstacles.\n",
        "\n",
        "**Challenge:** Rewards are sparse (+1 at goal, 0 elsewhere), making gradient-based learning difficult.\n",
        "\n",
        "**Approach:** Compare parameter-space optimization (ES) vs. action-space RL (PPO)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Formulation\n",
        "\n",
        "**Objective:**\n",
        "$$\\max_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T r_t \\right]$$\n",
        "\n",
        "**Evolution Strategies Gradient Estimate:**\n",
        "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N\\sigma} \\sum_{i=1}^N R(\\theta + \\sigma \\epsilon_i) \\cdot \\epsilon_i$$\n",
        "\n",
        "where $\\epsilon_i \\sim \\mathcal{N}(0, I)$\n",
        "\n",
        "**Update Rule:**\n",
        "$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\nabla_\\theta J(\\theta_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Requirements\n",
        "\n",
        "**Environment:**\n",
        "- State space: 64-dim (8×8 grid, one-hot encoded)\n",
        "- Action space: 4 discrete actions {up, down, left, right}\n",
        "- Episode length: max 50 steps\n",
        "- Obstacles: 8 randomly placed\n",
        "\n",
        "**Training Configuration:**\n",
        "- ES: 80 iterations, 50 perturbations × 5 episodes = 250 episodes per iteration\n",
        "- PPO: 80 iterations, 128 steps per rollout\n",
        "- Both methods train on shaped rewards (distance-based guidance)\n",
        "- Both evaluated on sparse rewards (0/+1 only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Success Metrics\n",
        "\n",
        "1. **Success Rate:** % of episodes reaching goal (target: >30%)\n",
        "2. **Average Return:** Mean cumulative reward\n",
        "3. **Learning Stability:** Std dev across trials (lower is better)\n",
        "4. **Sample Efficiency:** Iterations to reach threshold performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports successful!\n",
            "PyTorch version: 2.10.0+cpu\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# All required imports\n",
        "import sys\n",
        "sys.path.append('../src')  # Add src to path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 4)\n",
        "\n",
        "print(\"Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: 8×8 grid\n",
            "State shape: (64,)\n",
            "Action space: 4 actions\n",
            "Start position: (7, 0)\n",
            "Goal position: (0, 7)\n",
            "Number of obstacles: 8\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from model import GridWorld\n",
        "\n",
        "# Test environment\n",
        "env = GridWorld(size=8, n_obstacles=8, max_steps=50, seed=42)\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Environment: {env.size}×{env.size} grid\")\n",
        "print(f\"State shape: {state.shape}\")\n",
        "print(f\"Action space: {env.n_actions} actions\")\n",
        "print(f\"Start position: {env.start_pos}\")\n",
        "print(f\"Goal position: {env.goal_pos}\")\n",
        "print(f\"Number of obstacles: {len(env.obstacles)}\")\n",
        "\n",
        "# Visualize environment\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Policy Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy Network:\n",
            "  Input dim: 64\n",
            "  Hidden dim: 64\n",
            "  Output dim: 4\n",
            "  Layers: 2\n",
            "  Total parameters: 8580\n",
            "\n",
            "Test forward pass:\n",
            "  Input shape: torch.Size([1, 64])\n",
            "  Output shape: torch.Size([1, 4])\n",
            "  Action probs: [0.26338303 0.22026722 0.32858384 0.18776594]\n",
            "  Sum: 1.000000\n"
          ]
        }
      ],
      "source": [
        "from model import PolicyNetwork\n",
        "\n",
        "# Create policy network\n",
        "state_dim = env._get_state().shape[0]  # 64 for 8×8 grid\n",
        "action_dim = env.n_actions  # 4\n",
        "hidden_dim = 64\n",
        "n_layers = 2\n",
        "\n",
        "policy = PolicyNetwork(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    n_layers=n_layers\n",
        ")\n",
        "\n",
        "print(f\"Policy Network:\")\n",
        "print(f\"  Input dim: {state_dim}\")\n",
        "print(f\"  Hidden dim: {hidden_dim}\")\n",
        "print(f\"  Output dim: {action_dim}\")\n",
        "print(f\"  Layers: {n_layers}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in policy.parameters())}\")\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    logits = policy(state_tensor)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    print(f\"\\nTest forward pass:\")\n",
        "    print(f\"  Input shape: {state_tensor.shape}\")\n",
        "    print(f\"  Output shape: {logits.shape}\")\n",
        "    print(f\"  Action probs: {probs.squeeze().numpy()}\")\n",
        "    print(f\"  Sum: {probs.sum().item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evolution Strategies Implementation\n",
        "\n",
        "ES optimizes by perturbing parameters and estimating gradients from fitness evaluations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ES functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "from utils import sample_perturbation, score_function\n",
        "\n",
        "\n",
        "def evaluate_policy(policy, env, n_episodes=5, max_steps=50):\n",
        "    \"\"\"Evaluate policy and return average reward.\"\"\"\n",
        "    total_reward = 0.0\n",
        "    \n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0.0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            action, _ = policy.get_action(state, deterministic=False)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "        \n",
        "        total_reward += episode_reward\n",
        "    \n",
        "    return total_reward / n_episodes\n",
        "\n",
        "\n",
        "def es_step(policy, env, N=20, sigma=0.05, n_eval_episodes=5, max_steps=50,\n",
        "            noise_type='gaussian'):\n",
        "    \"\"\"\n",
        "    Single ES optimization step.\n",
        "    \n",
        "    Args:\n",
        "        policy: PolicyNetwork to optimize\n",
        "        env: Environment for evaluation\n",
        "        N: Population size\n",
        "        sigma: Noise scale\n",
        "        n_eval_episodes: Episodes per perturbation\n",
        "        max_steps: Max steps per episode\n",
        "        noise_type: Perturbation distribution — 'gaussian', 'cauchy', or 'laplace'\n",
        "    \n",
        "    Returns:\n",
        "        gradient: Estimated gradient\n",
        "        avg_reward: Average reward across population\n",
        "    \"\"\"\n",
        "    # Get flattened parameters\n",
        "    params = torch.cat([p.flatten() for p in policy.parameters()])\n",
        "    n_params = params.shape[0]\n",
        "    \n",
        "    # Sample perturbations and evaluate\n",
        "    perturbations = []\n",
        "    rewards = []\n",
        "    \n",
        "    for i in range(N):\n",
        "        # Sample perturbation from chosen distribution\n",
        "        epsilon = sample_perturbation(n_params, noise_type)\n",
        "        perturbations.append(epsilon)\n",
        "        \n",
        "        # Perturb parameters\n",
        "        perturbed_params = params + sigma * epsilon\n",
        "        \n",
        "        # Set perturbed parameters\n",
        "        offset = 0\n",
        "        for p in policy.parameters():\n",
        "            numel = p.numel()\n",
        "            p.data = perturbed_params[offset:offset+numel].view_as(p)\n",
        "            offset += numel\n",
        "        \n",
        "        # Evaluate\n",
        "        reward = evaluate_policy(policy, env, n_eval_episodes, max_steps)\n",
        "        rewards.append(reward)\n",
        "    \n",
        "    # Estimate gradient\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    perturbations = torch.stack(perturbations)\n",
        "    \n",
        "    # Apply the correct score function for the chosen noise distribution\n",
        "    score_weights = score_function(perturbations, noise_type)\n",
        "    \n",
        "    # Store original average reward before standardization\n",
        "    avg_reward = rewards.mean().item()\n",
        "    \n",
        "    # Standardize rewards for stability (only if there's variance)\n",
        "    if rewards.std() > 1e-8:\n",
        "        rewards_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
        "    else:\n",
        "        # If no variance, use raw rewards (no gradient signal, but at least not NaN)\n",
        "        rewards_normalized = rewards - rewards.mean()\n",
        "    \n",
        "    gradient = (score_weights.T @ rewards_normalized) / (N * sigma)\n",
        "    \n",
        "    # Restore original parameters\n",
        "    offset = 0\n",
        "    for p in policy.parameters():\n",
        "        numel = p.numel()\n",
        "        p.data = params[offset:offset+numel].view_as(p)\n",
        "        offset += numel\n",
        "    \n",
        "    return gradient, avg_reward\n",
        "\n",
        "\n",
        "print(\"ES functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[1/3] Training ES with gaussian perturbation noise...\n",
            "  gaussian ES iter 0/80 - train_reward: -0.217\n",
            "  gaussian ES iter 10/80 - train_reward: 2.515\n",
            "  gaussian ES iter 20/80 - train_reward: 3.079\n",
            "  gaussian ES iter 30/80 - train_reward: 1.012\n",
            "  gaussian ES iter 40/80 - train_reward: 3.454\n",
            "  gaussian ES iter 50/80 - train_reward: 3.542\n",
            "  gaussian ES iter 60/80 - train_reward: 3.480\n",
            "  gaussian ES iter 70/80 - train_reward: 3.607\n",
            "\n",
            " gaussian ES: reward=1.000, success=100.00%, time=918.4s\n",
            "\n",
            "[2/3] Training ES with cauchy perturbation noise...\n",
            "  cauchy ES iter 0/80 - train_reward: -0.486\n",
            "  cauchy ES iter 10/80 - train_reward: -0.433\n",
            "  cauchy ES iter 20/80 - train_reward: -1.046\n",
            "  cauchy ES iter 30/80 - train_reward: -0.611\n",
            "  cauchy ES iter 40/80 - train_reward: -0.485\n",
            "  cauchy ES iter 50/80 - train_reward: -0.491\n",
            "  cauchy ES iter 60/80 - train_reward: -0.504\n",
            "  cauchy ES iter 70/80 - train_reward: -0.388\n",
            "\n",
            " cauchy ES: reward=0.000, success=0.00%, time=291.3s\n",
            "\n",
            "[3/3] Training ES with laplace perturbation noise...\n",
            "  laplace ES iter 0/80 - train_reward: -0.185\n",
            "  laplace ES iter 10/80 - train_reward: 2.207\n",
            "  laplace ES iter 20/80 - train_reward: 3.039\n",
            "  laplace ES iter 30/80 - train_reward: 2.560\n",
            "  laplace ES iter 40/80 - train_reward: 2.757\n",
            "  laplace ES iter 50/80 - train_reward: 2.636\n",
            "  laplace ES iter 60/80 - train_reward: 2.447\n",
            "  laplace ES iter 70/80 - train_reward: 3.052\n",
            "\n",
            " laplace ES: reward=1.000, success=100.00%, time=360.5s\n",
            "\n",
            "Training PPO...\n",
            "Iter 1/80: train_reward=-0.850, eval_reward=-0.500, eval_success=0.00, eval_steps=50.0, ema=-0.850, best=-0.850\n",
            "Iter 10/80: train_reward=-0.200, eval_reward=-4.900, eval_success=0.00, eval_steps=50.0, ema=-0.197, best=2.260\n",
            "Iter 20/80: train_reward=3.202, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=1.476, best=3.202\n",
            "Iter 30/80: train_reward=0.660, eval_reward=1.900, eval_success=0.00, eval_steps=50.0, ema=2.037, best=3.360\n",
            "Iter 40/80: train_reward=3.224, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=2.674, best=3.360\n",
            "Iter 50/80: train_reward=3.322, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=3.048, best=3.540\n",
            "Iter 60/80: train_reward=3.348, eval_reward=1.900, eval_success=0.00, eval_steps=50.0, ema=3.181, best=3.540\n",
            "Iter 70/80: train_reward=3.577, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=3.404, best=3.598\n",
            "Iter 80/80: train_reward=3.490, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=3.402, best=3.598\n",
            "\n",
            "PPO: reward=1.000, success=100.00%, time=14.7s\n",
            "\n",
            "============================================================\n",
            "COMPARISON SUMMARY\n",
            "============================================================\n",
            "Method               Avg Reward      Success Rate    Time      \n",
            "------------------------------------------------------------\n",
            "ES-gaussian           1.000 ± 0.000  100.0% ±  0.0%   918.4s\n",
            "ES-cauchy             0.000 ± 0.000    0.0% ±  0.0%   291.3s\n",
            "ES-laplace            1.000 ± 0.000  100.0% ±  0.0%   360.5s\n",
            "PPO                   1.000 ± 0.000  100.0% ±  0.0%    14.7s\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jrtam\\AppData\\Local\\Temp\\ipykernel_11308\\1173458422.py:154: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "# ES vs PPO: Fair Comparison (8×8 grid, 8 obstacles, weaker shaping)\n",
        "from model import ValueNetwork\n",
        "from ppo_training import train_ppo\n",
        "from utils import NOISE_TYPES\n",
        "\n",
        "# Define shaped reward GridWorld (inherits from GridWorld)\n",
        "class ShapedRewardEnvComparison(GridWorld):\n",
        "    \"\"\"GridWorld with distance-based reward shaping.\"\"\"\n",
        "    def reset(self):\n",
        "        state = super().reset()\n",
        "        self.prev_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
        "        return state\n",
        "    \n",
        "    def step(self, action):\n",
        "        state, reward, done, info = super().step(action)\n",
        "        curr_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
        "        shaped_reward = reward + 0.2 * (self.prev_dist - curr_dist) - 0.01  # Weaker shaping\n",
        "        self.prev_dist = curr_dist\n",
        "        return state, shaped_reward, done, info\n",
        "\n",
        "# Use 8x8 with 8 obstacles (challenging)\n",
        "comparison_env = ShapedRewardEnvComparison(size=8, n_obstacles=8, max_steps=50, seed=123)\n",
        "eval_env = GridWorld(size=8, n_obstacles=8, max_steps=50, seed=123)\n",
        "\n",
        "# 1. Train ES\n",
        "iters = 80\n",
        "import time\n",
        "es_noise_rewards = dict()\n",
        "es_noise_successes = dict()\n",
        "es_noise_time = dict()\n",
        "for noise_idx, noise in enumerate(NOISE_TYPES):\n",
        "    print(f\"\\n[{noise_idx + 1}/{len(NOISE_TYPES)}] Training ES with {noise} perturbation noise...\")\n",
        "    es_start_time = time.time()\n",
        "    es_policy = PolicyNetwork(state_dim=64, action_dim=4, hidden_dim=64, n_layers=2)\n",
        "    es_params = torch.cat([p.flatten() for p in es_policy.parameters()])\n",
        "\n",
        "    for iteration in range(iters):\n",
        "        gradient, train_reward = es_step(es_policy, comparison_env, N=50, sigma=0.1, n_eval_episodes=5, max_steps=50, noise_type=noise)\n",
        "        es_params = es_params + 0.05 * gradient\n",
        "        \n",
        "        offset = 0\n",
        "        for p in es_policy.parameters():\n",
        "            numel = p.numel()\n",
        "            p.data = es_params[offset:offset+numel].view_as(p)\n",
        "            offset += numel\n",
        "        \n",
        "        if iteration % 10 == 0:\n",
        "            print(f\"  {noise} ES iter {iteration}/{iters} - train_reward: {train_reward:.3f}\")\n",
        "\n",
        "    # Evaluate ES\n",
        "    es_rewards = []\n",
        "    es_successes = []\n",
        "    for _ in range(20):\n",
        "        state = eval_env.reset()\n",
        "        ep_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        while not done and steps < 50:\n",
        "            action, _ = es_policy.get_action(state, deterministic=True)\n",
        "            state, reward, done, info = eval_env.step(action)\n",
        "            ep_reward += reward\n",
        "            steps += 1\n",
        "        es_rewards.append(ep_reward)\n",
        "        es_successes.append(float(info['success']))\n",
        "\n",
        "    es_time = time.time() - es_start_time\n",
        "    print(f\"\\n {noise} ES: reward={np.mean(es_rewards):.3f}, success={np.mean(es_successes):.2%}, time={es_time:.1f}s\")\n",
        "\n",
        "    # Save results for current ES noise\n",
        "    es_noise_rewards[noise] = es_rewards\n",
        "    es_noise_successes[noise] = es_successes\n",
        "    es_noise_time[noise] = es_time\n",
        "\n",
        "\n",
        "# 2. Train PPO\n",
        "print(\"\\nTraining PPO...\")\n",
        "ppo_start_time = time.time()\n",
        "ppo_policy = PolicyNetwork(state_dim=64, action_dim=4, hidden_dim=64, n_layers=2)\n",
        "ppo_value = ValueNetwork(state_dim=64, hidden_dim=64, n_layers=2)\n",
        "\n",
        "# PPO also trains on shaped rewards (ShapedRewardEnvComparison)\n",
        "trained_ppo_policy, _ = train_ppo(\n",
        "    policy=ppo_policy,\n",
        "    value_net=ppo_value,\n",
        "    env_class=ShapedRewardEnvComparison,\n",
        "    env_kwargs={\"size\": 8, \"n_obstacles\": 8, \"max_steps\": 50, \"seed\": 123},\n",
        "    n_iterations=80,\n",
        "    n_steps=128,\n",
        "    n_epochs=4,\n",
        "    batch_size=64,\n",
        "    lr_policy=3e-4,\n",
        "    lr_value=1e-3,\n",
        "    eval_every=10,\n",
        "    log_wandb=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Evaluate PPO\n",
        "ppo_rewards = []\n",
        "ppo_successes = []\n",
        "for _ in range(20):\n",
        "    state = eval_env.reset()\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < 50:\n",
        "        action, _ = trained_ppo_policy.get_action(state, deterministic=True)\n",
        "        state, reward, done, info = eval_env.step(action)\n",
        "        ep_reward += reward\n",
        "        steps += 1\n",
        "    ppo_rewards.append(ep_reward)\n",
        "    ppo_successes.append(float(info['success']))\n",
        "\n",
        "ppo_time = time.time() - ppo_start_time\n",
        "print(f\"\\nPPO: reward={np.mean(ppo_rewards):.3f}, success={np.mean(ppo_successes):.2%}, time={ppo_time:.1f}s\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Method':<20} {'Avg Reward':<15} {'Success Rate':<15} {'Time':<10}\")\n",
        "print(\"-\"*60)\n",
        "for noise in NOISE_TYPES:\n",
        "    label = f\"ES-{noise}\"\n",
        "    r = es_noise_rewards[noise]\n",
        "    s = es_noise_successes[noise]\n",
        "    t = es_noise_time[noise]\n",
        "    print(f\"{label:<20} {np.mean(r):>6.3f} ± {np.std(r):>5.3f}  {np.mean(s):>6.1%} ± {np.std(s):>5.1%}  {t:>6.1f}s\")\n",
        "print(f\"{'PPO':<20} {np.mean(ppo_rewards):>6.3f} ± {np.std(ppo_rewards):>5.3f}  {np.mean(ppo_successes):>6.1%} ± {np.std(ppo_successes):>5.1%}  {ppo_time:>6.1f}s\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize comparison\n",
        "box_data = [es_noise_rewards[n] for n in NOISE_TYPES] + [ppo_rewards]\n",
        "box_labels = [f\"ES-{n}\" for n in NOISE_TYPES] + [\"PPO\"]\n",
        "\n",
        "bar_names = [f\"ES-{n}\" for n in NOISE_TYPES] + [\"PPO\"]\n",
        "bar_values = [np.mean(es_noise_successes[n]) for n in NOISE_TYPES] + [np.mean(ppo_successes)]\n",
        "bar_colors = ['blue', 'purple', 'teal', 'orange']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].boxplot(box_data, tick_labels=box_labels)\n",
        "axes[0].set_ylabel('Reward')\n",
        "axes[0].set_title('Reward Distribution')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(bar_names, bar_values, color=bar_colors, alpha=0.6)\n",
        "axes[1].set_ylabel('Success Rate')\n",
        "axes[1].set_title('Success Rate Comparison')\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Resource Monitoring\n",
        "\n",
        "Track computational resources used by each method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESOURCE MONITORING\n",
            "============================================================\n",
            "\n",
            "Memory: 333.9 MB\n",
            "CPU cores: 12\n",
            "CPU usage: 26.9%\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESOURCE MONITORING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "process = psutil.Process(os.getpid())\n",
        "memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "print(f\"\\nMemory: {memory_mb:.1f} MB\")\n",
        "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
        "print(f\"CPU usage: {psutil.cpu_percent(interval=1):.1f}%\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
