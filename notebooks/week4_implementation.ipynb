{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 Implementation: Evolution Strategies for Non-Differentiable RL\n",
        "\n",
        "**Date:** February 6, 2026  \n",
        "**Course:** STAT 4830\n",
        "\n",
        "This notebook demonstrates a working implementation comparing Evolution Strategies (ES) with PPO on sparse reward gridworld environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Setup\n",
        "\n",
        "### Clear Problem Statement\n",
        "\n",
        "**Goal:** Learn a policy œÄ_Œ∏ that navigates from bottom-left to top-right in a gridworld with obstacles.\n",
        "\n",
        "**Challenge:** Rewards are sparse (+1 at goal, 0 elsewhere), making gradient-based learning difficult.\n",
        "\n",
        "**Approach:** Compare parameter-space optimization (ES) vs. action-space RL (PPO)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Formulation\n",
        "\n",
        "**Objective:**\n",
        "$$\\max_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T r_t \\right]$$\n",
        "\n",
        "**Evolution Strategies Gradient Estimate:**\n",
        "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N\\sigma} \\sum_{i=1}^N R(\\theta + \\sigma \\epsilon_i) \\cdot \\epsilon_i$$\n",
        "\n",
        "where $\\epsilon_i \\sim \\mathcal{N}(0, I)$\n",
        "\n",
        "**Update Rule:**\n",
        "$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\nabla_\\theta J(\\theta_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Requirements\n",
        "\n",
        "**Environment:**\n",
        "- State space: 64-dim (8√ó8 grid, one-hot encoded)\n",
        "- Action space: 4 discrete actions {up, down, left, right}\n",
        "- Episode length: max 50 steps\n",
        "- Obstacles: 8 randomly placed\n",
        "\n",
        "**Training Data:**\n",
        "- Generated online through policy rollouts\n",
        "- ES: 20 perturbations √ó 5 episodes = 100 episodes per iteration\n",
        "- PPO: 128 steps per rollout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Success Metrics\n",
        "\n",
        "1. **Success Rate:** % of episodes reaching goal (target: >30%)\n",
        "2. **Average Return:** Mean cumulative reward\n",
        "3. **Learning Stability:** Std dev across trials (lower is better)\n",
        "4. **Sample Efficiency:** Iterations to reach threshold performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports successful!\n",
            "PyTorch version: 2.8.0\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# All required imports\n",
        "import sys\n",
        "sys.path.append('../src')  # Add src to path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 4)\n",
        "\n",
        "print(\"Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: 8√ó8 grid\n",
            "State shape: (64,)\n",
            "Action space: 4 actions\n",
            "Start position: (7, 0)\n",
            "Goal position: (0, 7)\n",
            "Number of obstacles: 8\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from model import GridWorld\n",
        "\n",
        "# Test environment\n",
        "env = GridWorld(size=8, n_obstacles=8, max_steps=50, seed=42)\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Environment: {env.size}√ó{env.size} grid\")\n",
        "print(f\"State shape: {state.shape}\")\n",
        "print(f\"Action space: {env.n_actions} actions\")\n",
        "print(f\"Start position: {env.start_pos}\")\n",
        "print(f\"Goal position: {env.goal_pos}\")\n",
        "print(f\"Number of obstacles: {len(env.obstacles)}\")\n",
        "\n",
        "# Visualize environment\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Policy Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy Network:\n",
            "  Input dim: 64\n",
            "  Hidden dim: 64\n",
            "  Output dim: 4\n",
            "  Layers: 2\n",
            "  Total parameters: 8580\n",
            "\n",
            "Test forward pass:\n",
            "  Input shape: torch.Size([1, 64])\n",
            "  Output shape: torch.Size([1, 4])\n",
            "  Action probs: [0.27932164 0.22664662 0.20861788 0.28541377]\n",
            "  Sum: 1.000000\n"
          ]
        }
      ],
      "source": [
        "from model import PolicyNetwork\n",
        "\n",
        "# Create policy network\n",
        "state_dim = env._get_state().shape[0]  # 64 for 8√ó8 grid\n",
        "action_dim = env.n_actions  # 4\n",
        "hidden_dim = 64\n",
        "n_layers = 2\n",
        "\n",
        "policy = PolicyNetwork(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    n_layers=n_layers\n",
        ")\n",
        "\n",
        "print(f\"Policy Network:\")\n",
        "print(f\"  Input dim: {state_dim}\")\n",
        "print(f\"  Hidden dim: {hidden_dim}\")\n",
        "print(f\"  Output dim: {action_dim}\")\n",
        "print(f\"  Layers: {n_layers}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in policy.parameters())}\")\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    logits = policy(state_tensor)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    print(f\"\\nTest forward pass:\")\n",
        "    print(f\"  Input shape: {state_tensor.shape}\")\n",
        "    print(f\"  Output shape: {logits.shape}\")\n",
        "    print(f\"  Action probs: {probs.squeeze().numpy()}\")\n",
        "    print(f\"  Sum: {probs.sum().item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evolution Strategies Implementation\n",
        "\n",
        "ES optimizes by perturbing parameters and estimating gradients from fitness evaluations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ES functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def evaluate_policy(policy, env, n_episodes=5, max_steps=50):\n",
        "    \"\"\"Evaluate policy and return average reward.\"\"\"\n",
        "    total_reward = 0.0\n",
        "    \n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0.0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            action, _ = policy.get_action(state, deterministic=False)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "        \n",
        "        total_reward += episode_reward\n",
        "    \n",
        "    return total_reward / n_episodes\n",
        "\n",
        "\n",
        "def es_step(policy, env, N=20, sigma=0.05, n_eval_episodes=5, max_steps=50):\n",
        "    \"\"\"\n",
        "    Single ES optimization step.\n",
        "    \n",
        "    Args:\n",
        "        policy: PolicyNetwork to optimize\n",
        "        env: Environment for evaluation\n",
        "        N: Population size\n",
        "        sigma: Noise scale\n",
        "        n_eval_episodes: Episodes per perturbation\n",
        "        max_steps: Max steps per episode\n",
        "    \n",
        "    Returns:\n",
        "        gradient: Estimated gradient\n",
        "        avg_reward: Average reward across population\n",
        "    \"\"\"\n",
        "    # Get flattened parameters\n",
        "    params = torch.cat([p.flatten() for p in policy.parameters()])\n",
        "    n_params = params.shape[0]\n",
        "    \n",
        "    # Sample perturbations and evaluate\n",
        "    perturbations = []\n",
        "    rewards = []\n",
        "    \n",
        "    for i in range(N):\n",
        "        # Sample perturbation\n",
        "        epsilon = torch.randn(n_params)\n",
        "        perturbations.append(epsilon)\n",
        "        \n",
        "        # Perturb parameters\n",
        "        perturbed_params = params + sigma * epsilon\n",
        "        \n",
        "        # Set perturbed parameters\n",
        "        offset = 0\n",
        "        for p in policy.parameters():\n",
        "            numel = p.numel()\n",
        "            p.data = perturbed_params[offset:offset+numel].view_as(p)\n",
        "            offset += numel\n",
        "        \n",
        "        # Evaluate\n",
        "        reward = evaluate_policy(policy, env, n_eval_episodes, max_steps)\n",
        "        rewards.append(reward)\n",
        "    \n",
        "    # Estimate gradient\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    perturbations = torch.stack(perturbations)\n",
        "    \n",
        "    # Store original average reward before standardization\n",
        "    avg_reward = rewards.mean().item()\n",
        "    \n",
        "    # Standardize rewards for stability (only if there's variance)\n",
        "    if rewards.std() > 1e-8:\n",
        "        rewards_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
        "    else:\n",
        "        # If no variance, use raw rewards (no gradient signal, but at least not NaN)\n",
        "        rewards_normalized = rewards - rewards.mean()\n",
        "    \n",
        "    gradient = (perturbations.T @ rewards_normalized) / (N * sigma)\n",
        "    \n",
        "    # Restore original parameters\n",
        "    offset = 0\n",
        "    for p in policy.parameters():\n",
        "        numel = p.numel()\n",
        "        p.data = params[offset:offset+numel].view_as(p)\n",
        "        offset += numel\n",
        "    \n",
        "    return gradient, avg_reward\n",
        "\n",
        "\n",
        "print(\"ES functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: wandb not available (AttributeError). Install/reinstall with 'pip install --upgrade wandb' to enable logging.\n",
            "\n",
            "[1/2] Training ES...\n",
            "  ES iter 0/80 - train_reward: 0.198\n",
            "  ES iter 20/80 - train_reward: 1.882\n",
            "  ES iter 40/80 - train_reward: 3.409\n",
            "  ES iter 60/80 - train_reward: 3.660\n",
            "ES Results: reward=1.000, success=100.00%\n",
            "\n",
            "[2/2] Training PPO...\n",
            "Starting PPO training for 80 iterations...\n",
            "Policy parameters: 8580\n",
            "Value parameters: 8385\n",
            "Iter 1/80: train_reward=0.050, eval_reward=-0.500, eval_success=0.00, eval_steps=50.0, ema=0.050, best=0.050\n",
            "Iter 10/80: train_reward=0.450, eval_reward=0.900, eval_success=0.00, eval_steps=50.0, ema=-0.049, best=1.600\n",
            "Iter 20/80: train_reward=1.960, eval_reward=0.900, eval_success=0.00, eval_steps=50.0, ema=0.647, best=2.987\n",
            "Iter 30/80: train_reward=2.913, eval_reward=1.300, eval_success=0.00, eval_steps=50.0, ema=1.637, best=2.987\n",
            "Iter 40/80: train_reward=1.763, eval_reward=1.300, eval_success=0.00, eval_steps=50.0, ema=2.260, best=3.210\n",
            "Iter 50/80: train_reward=3.365, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=2.791, best=3.514\n",
            "Iter 60/80: train_reward=3.450, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=3.087, best=3.514\n",
            "Iter 70/80: train_reward=2.674, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=3.239, best=3.590\n",
            "Iter 80/80: train_reward=3.523, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=3.450, best=3.611\n",
            "\n",
            "Training complete! Best reward: 3.611\n",
            "PPO Results: reward=1.000, success=100.00%\n",
            "\n",
            "============================================================\n",
            "COMPARISON SUMMARY\n",
            "============================================================\n",
            "Method     Avg Reward      Success Rate   \n",
            "------------------------------------------------------------\n",
            "ES          1.000 ¬± 0.000  100.0% ¬±  0.0%\n",
            "PPO         1.000 ¬± 0.000  100.0% ¬±  0.0%\n",
            "\n",
            "============================================================\n",
            "KEY INSIGHTS\n",
            "============================================================\n",
            "‚úÖ Learning Successful! Methods navigating challenging gridworld\n",
            "   ES:  100.0% success rate\n",
            "   PPO: 100.0% success rate\n",
            "\n",
            "üìä Comparison:\n",
            "   Both methods performed similarly\n",
            "\n",
            "üí° Key Takeaway:\n",
            "   BOTH ES and PPO train on SHAPED rewards (fair comparison)\n",
            "   Both evaluated on SPARSE rewards (true performance)\n",
            "   ES: simpler but less sample-efficient\n",
            "   PPO: more complex but uses gradients effectively\n",
            "\n",
            "‚úì Comparison complete!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/xn/81b_v12s5t5317x0pksjtnxr0000gn/T/ipykernel_95431/2801470684.py:155: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "# ES vs PPO: Fair Comparison (8√ó8 grid, 8 obstacles, weaker shaping)\n",
        "import sys\n",
        "if '../tiny-grpo-es' not in sys.path:\n",
        "    sys.path.append('../tiny-grpo-es')\n",
        "from policy_network import ValueNetwork\n",
        "from train_ppo_gridworld import train_ppo\n",
        "\n",
        "# Define shaped reward GridWorld (inherits from GridWorld)\n",
        "class ShapedRewardEnvComparison(GridWorld):\n",
        "    \"\"\"GridWorld with distance-based reward shaping.\"\"\"\n",
        "    def reset(self):\n",
        "        state = super().reset()\n",
        "        self.prev_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
        "        return state\n",
        "    \n",
        "    def step(self, action):\n",
        "        state, reward, done, info = super().step(action)\n",
        "        curr_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
        "        shaped_reward = reward + 0.2 * (self.prev_dist - curr_dist) - 0.01  # Weaker shaping\n",
        "        self.prev_dist = curr_dist\n",
        "        return state, shaped_reward, done, info\n",
        "\n",
        "# Use 8x8 with 8 obstacles (challenging)\n",
        "comparison_env = ShapedRewardEnvComparison(size=8, n_obstacles=8, max_steps=50, seed=123)\n",
        "eval_env = GridWorld(size=8, n_obstacles=8, max_steps=50, seed=123)\n",
        "\n",
        "# 1. Train ES\n",
        "print(\"\\n[1/2] Training ES...\")\n",
        "import time\n",
        "es_start_time = time.time()\n",
        "es_policy = PolicyNetwork(state_dim=64, action_dim=4, hidden_dim=64, n_layers=2)\n",
        "es_params = torch.cat([p.flatten() for p in es_policy.parameters()])\n",
        "\n",
        "for iteration in range(80):\n",
        "    gradient, train_reward = es_step(es_policy, comparison_env, N=50, sigma=0.1, n_eval_episodes=5, max_steps=50)\n",
        "    es_params = es_params + 0.05 * gradient\n",
        "    \n",
        "    offset = 0\n",
        "    for p in es_policy.parameters():\n",
        "        numel = p.numel()\n",
        "        p.data = es_params[offset:offset+numel].view_as(p)\n",
        "        offset += numel\n",
        "    \n",
        "    if iteration % 20 == 0:\n",
        "        print(f\"  ES iter {iteration}/80 - train_reward: {train_reward:.3f}\")\n",
        "\n",
        "# Evaluate ES\n",
        "es_rewards = []\n",
        "es_successes = []\n",
        "for _ in range(20):\n",
        "    state = eval_env.reset()\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < 50:\n",
        "        action, _ = es_policy.get_action(state, deterministic=True)\n",
        "        state, reward, done, info = eval_env.step(action)\n",
        "        ep_reward += reward\n",
        "        steps += 1\n",
        "    es_rewards.append(ep_reward)\n",
        "    es_successes.append(float(info['success']))\n",
        "\n",
        "es_time = time.time() - es_start_time\n",
        "print(f\"\\nES: reward={np.mean(es_rewards):.3f}, success={np.mean(es_successes):.2%}, time={es_time:.1f}s\")\n",
        "\n",
        "# 2. Train PPO\n",
        "print(\"\\n[2/2] Training PPO...\")\n",
        "ppo_start_time = time.time()\n",
        "ppo_policy = PolicyNetwork(state_dim=64, action_dim=4, hidden_dim=64, n_layers=2)\n",
        "ppo_value = ValueNetwork(state_dim=64, hidden_dim=64, n_layers=2)\n",
        "\n",
        "# PPO also trains on shaped rewards (ShapedRewardEnvComparison)\n",
        "trained_ppo_policy, _ = train_ppo(\n",
        "    policy=ppo_policy,\n",
        "    value_net=ppo_value,\n",
        "    env_class=ShapedRewardEnvComparison,\n",
        "    env_kwargs={\"size\": 8, \"n_obstacles\": 8, \"max_steps\": 50, \"seed\": 123},\n",
        "    n_iterations=80,\n",
        "    n_steps=128,\n",
        "    n_epochs=4,\n",
        "    batch_size=64,\n",
        "    lr_policy=3e-4,\n",
        "    lr_value=1e-3,\n",
        "    eval_every=10,\n",
        "    log_wandb=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Evaluate PPO\n",
        "ppo_rewards = []\n",
        "ppo_successes = []\n",
        "for _ in range(20):\n",
        "    state = eval_env.reset()\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < 50:\n",
        "        action, _ = trained_ppo_policy.get_action(state, deterministic=True)\n",
        "        state, reward, done, info = eval_env.step(action)\n",
        "        ep_reward += reward\n",
        "        steps += 1\n",
        "    ppo_rewards.append(ep_reward)\n",
        "    ppo_successes.append(float(info['success']))\n",
        "\n",
        "ppo_time = time.time() - ppo_start_time\n",
        "print(f\"\\nPPO: reward={np.mean(ppo_rewards):.3f}, success={np.mean(ppo_successes):.2%}, time={ppo_time:.1f}s\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Method':<10} {'Avg Reward':<15} {'Success Rate':<15} {'Time':<10}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'ES':<10} {np.mean(es_rewards):>6.3f} ¬± {np.std(es_rewards):>5.3f}  {np.mean(es_successes):>6.1%} ¬± {np.std(es_successes):>5.1%}  {es_time:>6.1f}s\")\n",
        "print(f\"{'PPO':<10} {np.mean(ppo_rewards):>6.3f} ¬± {np.std(ppo_rewards):>5.3f}  {np.mean(ppo_successes):>6.1%} ¬± {np.std(ppo_successes):>5.1%}  {ppo_time:>6.1f}s\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "axes[0].boxplot([es_rewards, ppo_rewards], labels=['ES', 'PPO'])\n",
        "axes[0].set_ylabel('Reward')\n",
        "axes[0].set_title('Reward Distribution')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(['ES', 'PPO'], [np.mean(es_successes), np.mean(ppo_successes)], \n",
        "            color=['blue', 'orange'], alpha=0.6)\n",
        "axes[1].set_ylabel('Success Rate')\n",
        "axes[1].set_title('Success Rate Comparison')\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Resource Monitoring\n",
        "\n",
        "Track computational resources used by each method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESOURCE MONITORING\n",
            "============================================================\n",
            "\n",
            "üìä Current Resource Usage:\n",
            "   Memory: 122.0 MB\n",
            "   CPU cores available: 8\n",
            "   CPU percent: 20.3%\n",
            "\n",
            "‚è±Ô∏è  Training Configuration:\n",
            "   ES: 80 iterations √ó 50 population √ó 5 episodes = 20,000 episodes\n",
            "   PPO: 80 iterations √ó 128 steps = 10,240 steps\n",
            "   Environment: 8√ó8 grid, 8 obstacles, max 50 steps\n",
            "\n",
            "üí° Key Differences:\n",
            "   ES: More episodes (parameter perturbations)\n",
            "   PPO: More efficient (gradient-based, fewer episodes)\n",
            "   Both converge to 100% success on this task\n",
            "\n",
            "‚úì Resource monitoring complete!\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESOURCE MONITORING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "process = psutil.Process(os.getpid())\n",
        "memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "print(f\"\\nMemory: {memory_mb:.1f} MB\")\n",
        "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
        "print(f\"CPU usage: {psutil.cpu_percent(interval=1):.1f}%\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
