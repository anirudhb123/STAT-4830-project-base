{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 Implementation: Evolution Strategies for Non-Differentiable RL\n",
        "\n",
        "**Authors:** Anirudh Bharadwaj, Shaurya Singhi, Sid Srivastava, Roberto Tamez  \n",
        "**Date:** February 6, 2026  \n",
        "**Course:** STAT 4830\n",
        "\n",
        "This notebook demonstrates a working implementation of Evolution Strategies (ES) for sparse reward gridworld environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Setup\n",
        "\n",
        "### Clear Problem Statement\n",
        "\n",
        "**Goal:** Learn a policy π_θ that navigates from bottom-left to top-right in a gridworld with obstacles.\n",
        "\n",
        "**Challenge:** Rewards are sparse (+1 at goal, 0 elsewhere), making gradient-based learning difficult.\n",
        "\n",
        "**Approach:** Use parameter-space optimization via Evolution Strategies (ES), which does not require differentiable rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Formulation\n",
        "\n",
        "**Objective:**\n",
        "$$\\max_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T r_t \\right]$$\n",
        "\n",
        "**Evolution Strategies Gradient Estimate:**\n",
        "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N\\sigma} \\sum_{i=1}^N R(\\theta + \\sigma \\epsilon_i) \\cdot \\epsilon_i$$\n",
        "\n",
        "where $\\epsilon_i \\sim \\mathcal{N}(0, I)$\n",
        "\n",
        "**Update Rule:**\n",
        "$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\nabla_\\theta J(\\theta_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Requirements\n",
        "\n",
        "**Environment:**\n",
        "- State space: 64-dim (8×8 grid, one-hot encoded)\n",
        "- Action space: 4 discrete actions {up, down, left, right}\n",
        "- Episode length: max 50 steps\n",
        "- Obstacles: 8 randomly placed\n",
        "\n",
        "**Training Data:**\n",
        "- Generated online through policy rollouts\n",
        "- ES: 20 perturbations × 5 episodes = 100 episodes per iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Success Metrics\n",
        "\n",
        "1. **Success Rate:** % of episodes reaching goal (target: >30%)\n",
        "2. **Average Return:** Mean cumulative reward\n",
        "3. **Learning Stability:** Std dev across trials (lower is better)\n",
        "4. **Sample Efficiency:** Iterations to reach threshold performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports successful!\n",
            "PyTorch version: 2.8.0\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# All required imports\n",
        "import sys\n",
        "sys.path.append('../src')  # Add src to path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 4)\n",
        "\n",
        "print(\"Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: 8×8 grid\n",
            "State shape: (64,)\n",
            "Action space: 4 actions\n",
            "Start position: (7, 0)\n",
            "Goal position: (0, 7)\n",
            "Number of obstacles: 8\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from model import GridWorld\n",
        "\n",
        "# Test environment\n",
        "env = GridWorld(size=8, n_obstacles=8, max_steps=50, seed=42)\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Environment: {env.size}×{env.size} grid\")\n",
        "print(f\"State shape: {state.shape}\")\n",
        "print(f\"Action space: {env.n_actions} actions\")\n",
        "print(f\"Start position: {env.start_pos}\")\n",
        "print(f\"Goal position: {env.goal_pos}\")\n",
        "print(f\"Number of obstacles: {len(env.obstacles)}\")\n",
        "\n",
        "# Visualize environment\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Policy Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy Network:\n",
            "  Input dim: 64\n",
            "  Hidden dim: 64\n",
            "  Output dim: 4\n",
            "  Layers: 2\n",
            "  Total parameters: 8580\n",
            "\n",
            "Test forward pass:\n",
            "  Input shape: torch.Size([1, 64])\n",
            "  Output shape: torch.Size([1, 4])\n",
            "  Action probs: [0.22937416 0.3139211  0.21942699 0.23727772]\n",
            "  Sum: 1.000000\n"
          ]
        }
      ],
      "source": [
        "from model import PolicyNetwork\n",
        "\n",
        "# Create policy network\n",
        "state_dim = env._get_state().shape[0]  # 64 for 8×8 grid\n",
        "action_dim = env.n_actions  # 4\n",
        "hidden_dim = 64\n",
        "n_layers = 2\n",
        "\n",
        "policy = PolicyNetwork(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    n_layers=n_layers\n",
        ")\n",
        "\n",
        "print(f\"Policy Network:\")\n",
        "print(f\"  Input dim: {state_dim}\")\n",
        "print(f\"  Hidden dim: {hidden_dim}\")\n",
        "print(f\"  Output dim: {action_dim}\")\n",
        "print(f\"  Layers: {n_layers}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in policy.parameters())}\")\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    logits = policy(state_tensor)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    print(f\"\\nTest forward pass:\")\n",
        "    print(f\"  Input shape: {state_tensor.shape}\")\n",
        "    print(f\"  Output shape: {logits.shape}\")\n",
        "    print(f\"  Action probs: {probs.squeeze().numpy()}\")\n",
        "    print(f\"  Sum: {probs.sum().item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objective Function & Optimization Algorithm\n",
        "\n",
        "**Objective function:** `evaluate_policy` computes $J(\\theta) = \\mathbb{E}[\\sum r_t]$ by averaging episode returns under the current policy.\n",
        "\n",
        "**Optimization algorithm:** `es_step` implements one step of Evolution Strategies — perturbing parameters, evaluating fitness via the objective, and estimating the gradient from the population."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ES functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def evaluate_policy(policy, env, n_episodes=5, max_steps=50):\n",
        "    \"\"\"Evaluate policy and return average reward.\"\"\"\n",
        "    total_reward = 0.0\n",
        "    \n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0.0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            action, _ = policy.get_action(state, deterministic=False)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "        \n",
        "        total_reward += episode_reward\n",
        "    \n",
        "    return total_reward / n_episodes\n",
        "\n",
        "\n",
        "def es_step(policy, env, N=20, sigma=0.05, n_eval_episodes=5, max_steps=50):\n",
        "    \"\"\"\n",
        "    Single ES optimization step.\n",
        "    \n",
        "    Args:\n",
        "        policy: PolicyNetwork to optimize\n",
        "        env: Environment for evaluation\n",
        "        N: Population size\n",
        "        sigma: Noise scale\n",
        "        n_eval_episodes: Episodes per perturbation\n",
        "        max_steps: Max steps per episode\n",
        "    \n",
        "    Returns:\n",
        "        gradient: Estimated gradient\n",
        "        avg_reward: Average reward across population\n",
        "    \"\"\"\n",
        "    # Get flattened parameters\n",
        "    params = torch.cat([p.flatten() for p in policy.parameters()])\n",
        "    n_params = params.shape[0]\n",
        "    \n",
        "    # Sample perturbations and evaluate\n",
        "    perturbations = []\n",
        "    rewards = []\n",
        "    \n",
        "    for i in range(N):\n",
        "        # Sample perturbation\n",
        "        epsilon = torch.randn(n_params)\n",
        "        perturbations.append(epsilon)\n",
        "        \n",
        "        # Perturb parameters\n",
        "        perturbed_params = params + sigma * epsilon\n",
        "        \n",
        "        # Set perturbed parameters\n",
        "        offset = 0\n",
        "        for p in policy.parameters():\n",
        "            numel = p.numel()\n",
        "            p.data = perturbed_params[offset:offset+numel].view_as(p)\n",
        "            offset += numel\n",
        "        \n",
        "        # Evaluate\n",
        "        reward = evaluate_policy(policy, env, n_eval_episodes, max_steps)\n",
        "        rewards.append(reward)\n",
        "    \n",
        "    # Estimate gradient\n",
        "    rewards = torch.tensor(rewards)\n",
        "    perturbations = torch.stack(perturbations)\n",
        "    \n",
        "    # Standardize rewards for stability\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
        "    \n",
        "    gradient = (perturbations.T @ rewards) / (N * sigma)\n",
        "    \n",
        "    # Restore original parameters\n",
        "    offset = 0\n",
        "    for p in policy.parameters():\n",
        "        numel = p.numel()\n",
        "        p.data = params[offset:offset+numel].view_as(p)\n",
        "        offset += numel\n",
        "    \n",
        "    return gradient, rewards.mean().item()\n",
        "\n",
        "\n",
        "print(\"ES functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Parameters and Choices\n",
        "\n",
        "**ES Hyperparameters:**\n",
        "- Population size N = 20 (balance between gradient quality and computation)\n",
        "- Noise scale σ = 0.05 (small enough for local search, large enough for exploration)\n",
        "- Learning rate α = 0.01 (conservative to avoid instability)\n",
        "- Evaluation episodes = 5 per perturbation (reduce variance)\n",
        "\n",
        "**Design Choices:**\n",
        "1. Reward standardization: Improves gradient stability\n",
        "2. Parameter flattening: Easier gradient computation\n",
        "3. Multiple evaluation episodes: Reduces environment stochasticity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Logging/Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logger class defined!\n"
          ]
        }
      ],
      "source": [
        "class TrainingLogger:\n",
        "    \"\"\"Simple logger for tracking training progress.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.history = {\n",
        "            'iteration': [],\n",
        "            'avg_reward': [],\n",
        "            'success_rate': [],\n",
        "            'gradient_norm': []\n",
        "        }\n",
        "    \n",
        "    def log(self, iteration, avg_reward, success_rate, gradient_norm):\n",
        "        self.history['iteration'].append(iteration)\n",
        "        self.history['avg_reward'].append(avg_reward)\n",
        "        self.history['success_rate'].append(success_rate)\n",
        "        self.history['gradient_norm'].append(gradient_norm)\n",
        "    \n",
        "    def plot(self):\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "        \n",
        "        axes[0].plot(self.history['iteration'], self.history['avg_reward'])\n",
        "        axes[0].set_xlabel('Iteration')\n",
        "        axes[0].set_ylabel('Average Reward')\n",
        "        axes[0].set_title('Training Progress: Reward')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1].plot(self.history['iteration'], self.history['success_rate'])\n",
        "        axes[1].set_xlabel('Iteration')\n",
        "        axes[1].set_ylabel('Success Rate')\n",
        "        axes[1].set_title('Training Progress: Success Rate')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[2].plot(self.history['iteration'], self.history['gradient_norm'])\n",
        "        axes[2].set_xlabel('Iteration')\n",
        "        axes[2].set_ylabel('Gradient Norm')\n",
        "        axes[2].set_title('Gradient Magnitude')\n",
        "        axes[2].set_yscale('log')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "print(\"Logger class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation\n",
        "\n",
        "### Test Cases with Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 1: Environment Mechanics\n",
            "==================================================\n",
            "✓ Environment resets correctly\n",
            "  Start: (0, 0), Goal: (3, 3)\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Should reach goal",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not be done yet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m done, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould reach goal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be successful\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m reward \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould get goal reward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Should reach goal"
          ]
        }
      ],
      "source": [
        "# Test 1: Environment mechanics\n",
        "print(\"Test 1: Environment Mechanics\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "env = GridWorld(size=4, n_obstacles=0, max_steps=20, seed=42)\n",
        "env.start_pos = (0, 0)\n",
        "env.goal_pos = (3, 3)\n",
        "\n",
        "state = env.reset()\n",
        "print(f\"✓ Environment resets correctly\")\n",
        "print(f\"  Start: {env.start_pos}, Goal: {env.goal_pos}\")\n",
        "\n",
        "# Test actions\n",
        "actions = [1, 1, 1, 0, 0, 0]  # Right, Right, Right, Up, Up, Up\n",
        "for i, action in enumerate(actions):\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if i < len(actions) - 1:\n",
        "        assert not done, \"Should not be done yet\"\n",
        "    else:\n",
        "        assert done, \"Should reach goal\"\n",
        "        assert info['success'], \"Should be successful\"\n",
        "        assert reward == 1.0, \"Should get goal reward\"\n",
        "\n",
        "print(f\"✓ Action execution works correctly\")\n",
        "print(f\"✓ Goal detection works\")\n",
        "print(f\"✓ Reward structure correct\\n\")\n",
        "\n",
        "# Test 2: Policy network\n",
        "print(\"Test 2: Policy Network\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "policy = PolicyNetwork(state_dim=16, action_dim=4, hidden_dim=32, n_layers=2)\n",
        "state = torch.randn(1, 16)\n",
        "logits = policy(state)\n",
        "\n",
        "assert logits.shape == (1, 4), \"Output shape incorrect\"\n",
        "print(f\"✓ Forward pass shape correct: {logits.shape}\")\n",
        "\n",
        "action, log_prob = policy.get_action(state[0].numpy())\n",
        "assert 0 <= action < 4, \"Action out of bounds\"\n",
        "assert log_prob is not None, \"Log prob should be returned\"\n",
        "print(f\"✓ Action sampling works: action={action}\")\n",
        "print(f\"✓ Log prob computed: {log_prob:.4f}\\n\")\n",
        "\n",
        "# Test 3: ES gradient estimation\n",
        "print(\"Test 3: ES Gradient Estimation\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "env = GridWorld(size=4, n_obstacles=0, max_steps=20, seed=42)\n",
        "policy = PolicyNetwork(state_dim=16, action_dim=4, hidden_dim=32, n_layers=2)\n",
        "\n",
        "gradient, avg_reward = es_step(policy, env, N=5, sigma=0.05, n_eval_episodes=2, max_steps=20)\n",
        "\n",
        "n_params = sum(p.numel() for p in policy.parameters())\n",
        "assert gradient.shape[0] == n_params, \"Gradient shape incorrect\"\n",
        "print(f\"✓ Gradient shape correct: {gradient.shape}\")\n",
        "print(f\"✓ Average reward: {avg_reward:.4f}\")\n",
        "print(f\"✓ Gradient norm: {gradient.norm().item():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"All tests passed! ✓\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Measurements\n",
        "\n",
        "Quick training run to validate learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Training Run: 20 iterations\n",
            "==================================================\n",
            "Iter   0: reward= 0.000, success=0.00, grad_norm=405.1559\n",
            "Iter   5: reward= 0.000, success=0.00, grad_norm=401.7718\n",
            "Iter  10: reward= 0.000, success=0.00, grad_norm=404.9162\n",
            "Iter  15: reward= 0.000, success=0.00, grad_norm=402.5747\n",
            "\n",
            "Training complete!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/xn/81b_v12s5t5317x0pksjtnxr0000gn/T/ipykernel_76756/3020063817.py:41: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "# Quick training run (20 iterations)\n",
        "print(\"Quick Training Run: 20 iterations\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "env = GridWorld(size=8, n_obstacles=8, max_steps=50, seed=42)\n",
        "policy = PolicyNetwork(\n",
        "    state_dim=64,\n",
        "    action_dim=4,\n",
        "    hidden_dim=64,\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "logger = TrainingLogger()\n",
        "alpha = 0.01  # Learning rate\n",
        "N = 20  # Population size\n",
        "sigma = 0.05  # Noise scale\n",
        "\n",
        "# Get flattened parameters\n",
        "params = torch.cat([p.flatten() for p in policy.parameters()])\n",
        "\n",
        "for iteration in range(20):\n",
        "    # ES step\n",
        "    gradient, avg_reward = es_step(\n",
        "        policy, env,\n",
        "        N=N,\n",
        "        sigma=sigma,\n",
        "        n_eval_episodes=5,\n",
        "        max_steps=50\n",
        "    )\n",
        "    \n",
        "    # Update parameters\n",
        "    params = params + alpha * gradient\n",
        "    \n",
        "    # Set updated parameters\n",
        "    offset = 0\n",
        "    for p in policy.parameters():\n",
        "        numel = p.numel()\n",
        "        p.data = params[offset:offset+numel].view_as(p)\n",
        "        offset += numel\n",
        "    \n",
        "    # Evaluate\n",
        "    eval_rewards = []\n",
        "    eval_successes = []\n",
        "    for _ in range(10):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        \n",
        "        while not done and steps < 50:\n",
        "            action, _ = policy.get_action(state, deterministic=True)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "        \n",
        "        eval_rewards.append(episode_reward)\n",
        "        eval_successes.append(float(info['success']))\n",
        "    \n",
        "    success_rate = np.mean(eval_successes)\n",
        "    eval_reward = np.mean(eval_rewards)\n",
        "    grad_norm = gradient.norm().item()\n",
        "    \n",
        "    logger.log(iteration, eval_reward, success_rate, grad_norm)\n",
        "    \n",
        "    if iteration % 5 == 0:\n",
        "        print(f\"Iter {iteration:3d}: reward={eval_reward:6.3f}, success={success_rate:.2f}, grad_norm={grad_norm:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "logger.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resource Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resource Usage Analysis\n",
            "==================================================\n",
            "Single ES Iteration:\n",
            "  Time: 0.65 seconds\n",
            "  Memory delta: -17.25 MB\n",
            "  Current memory: 126.58 MB\n",
            "\n",
            "Estimated 100 iterations: 1.1 minutes\n",
            "\n",
            "Model Statistics:\n",
            "  Parameters: 8,580\n",
            "  Size: 0.033 MB\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "print(\"Resource Usage Analysis\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Time one iteration\n",
        "start_time = time.time()\n",
        "start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "gradient, avg_reward = es_step(\n",
        "    policy, env,\n",
        "    N=20,\n",
        "    sigma=0.05,\n",
        "    n_eval_episodes=5,\n",
        "    max_steps=50\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "end_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "iteration_time = end_time - start_time\n",
        "memory_used = end_memory - start_memory\n",
        "\n",
        "print(f\"Single ES Iteration:\")\n",
        "print(f\"  Time: {iteration_time:.2f} seconds\")\n",
        "print(f\"  Memory delta: {memory_used:.2f} MB\")\n",
        "print(f\"  Current memory: {end_memory:.2f} MB\")\n",
        "print(f\"\\nEstimated 100 iterations: {iteration_time * 100 / 60:.1f} minutes\")\n",
        "\n",
        "# Model size\n",
        "n_params = sum(p.numel() for p in policy.parameters())\n",
        "model_size_mb = n_params * 4 / 1024 / 1024  # 4 bytes per float32\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Parameters: {n_params:,}\")\n",
        "print(f\"  Size: {model_size_mb:.3f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Outputs\n",
        "\n",
        "Visualize learned policy behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visualizing learned policy behavior:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/xn/81b_v12s5t5317x0pksjtnxr0000gn/T/ipykernel_76756/2797500800.py:48: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "def visualize_policy(policy, env, n_episodes=3):\n",
        "    \"\"\"Visualize policy rollouts.\"\"\"\n",
        "    fig, axes = plt.subplots(1, n_episodes, figsize=(5*n_episodes, 5))\n",
        "    if n_episodes == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for ep, ax in enumerate(axes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        trajectory = [env.agent_pos]\n",
        "        \n",
        "        while not done and steps < 50:\n",
        "            action, _ = policy.get_action(state, deterministic=True)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            trajectory.append(env.agent_pos)\n",
        "            steps += 1\n",
        "        \n",
        "        # Plot grid\n",
        "        grid = np.zeros((env.size, env.size))\n",
        "        \n",
        "        # Mark obstacles\n",
        "        for obs in env.obstacles:\n",
        "            grid[obs] = -1\n",
        "        \n",
        "        # Mark trajectory\n",
        "        for i, pos in enumerate(trajectory):\n",
        "            if grid[pos] == 0:  # Don't overwrite obstacles\n",
        "                grid[pos] = (i + 1) / len(trajectory)\n",
        "        \n",
        "        # Mark start and goal\n",
        "        grid[env.start_pos] = 0.5\n",
        "        grid[env.goal_pos] = 1.0\n",
        "        \n",
        "        im = ax.imshow(grid, cmap='RdYlGn', vmin=-1, vmax=1)\n",
        "        ax.set_title(f\"Episode {ep+1}\\n{'Success' if info['success'] else 'Failed'} ({steps} steps)\")\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        \n",
        "        # Add markers\n",
        "        ax.plot(env.start_pos[1], env.start_pos[0], 'bo', markersize=15, label='Start')\n",
        "        ax.plot(env.goal_pos[1], env.goal_pos[0], 'g*', markersize=20, label='Goal')\n",
        "        \n",
        "        if ep == 0:\n",
        "            ax.legend(loc='upper left', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Visualizing learned policy behavior:\")\n",
        "visualize_policy(policy, env, n_episodes=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Edge Case Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Edge Case Testing\n",
            "==================================================\n",
            "\n",
            "Test 1: Empty grid (no obstacles)\n",
            "  Success rate: 0.00% (expect >80%)\n",
            "\n",
            "Test 2: Dense obstacles (15 obstacles on 8×8)\n",
            "  Success rate: 0.00% (expect >10%)\n",
            "\n",
            "Test 3: Larger grid (12×12)\n",
            "  Success rate (untrained): 0.00% (random baseline)\n",
            "\n",
            "==================================================\n",
            "Edge case testing complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"Edge Case Testing\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Edge Case 1: Empty grid (should achieve high success)\n",
        "print(\"\\nTest 1: Empty grid (no obstacles)\")\n",
        "env_empty = GridWorld(size=8, n_obstacles=0, max_steps=50, seed=42)\n",
        "success_count = 0\n",
        "for _ in range(20):\n",
        "    state = env_empty.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < 50:\n",
        "        action, _ = policy.get_action(state, deterministic=True)\n",
        "        state, reward, done, info = env_empty.step(action)\n",
        "        steps += 1\n",
        "    if info['success']:\n",
        "        success_count += 1\n",
        "\n",
        "print(f\"  Success rate: {success_count/20:.2%} (expect >80%)\")\n",
        "\n",
        "# Edge Case 2: Dense obstacles\n",
        "print(\"\\nTest 2: Dense obstacles (15 obstacles on 8×8)\")\n",
        "env_dense = GridWorld(size=8, n_obstacles=15, max_steps=50, seed=42)\n",
        "success_count = 0\n",
        "for _ in range(20):\n",
        "    state = env_dense.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < 50:\n",
        "        action, _ = policy.get_action(state, deterministic=True)\n",
        "        state, reward, done, info = env_dense.step(action)\n",
        "        steps += 1\n",
        "    if info['success']:\n",
        "        success_count += 1\n",
        "\n",
        "print(f\"  Success rate: {success_count/20:.2%} (expect >10%)\")\n",
        "\n",
        "# Edge Case 3: Larger grid\n",
        "print(\"\\nTest 3: Larger grid (12×12)\")\n",
        "env_large = GridWorld(size=12, n_obstacles=12, max_steps=100, seed=42)\n",
        "\n",
        "# Need to create new policy for different state dim\n",
        "policy_large = PolicyNetwork(\n",
        "    state_dim=144,  # 12×12\n",
        "    action_dim=4,\n",
        "    hidden_dim=64,\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "success_count = 0\n",
        "for _ in range(20):\n",
        "    state = env_large.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < 100:\n",
        "        action, _ = policy_large.get_action(state, deterministic=False)\n",
        "        state, reward, done, info = env_large.step(action)\n",
        "        steps += 1\n",
        "    if info['success']:\n",
        "        success_count += 1\n",
        "\n",
        "print(f\"  Success rate (untrained): {success_count/20:.2%} (random baseline)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Edge case testing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Design Decisions\n",
        "\n",
        "1. **One-hot state encoding:** Simple and works for small grids. For larger grids, could use embedding.\n",
        "\n",
        "2. **Reward standardization in ES:** Dividing by std helps with gradient stability when fitness values have different scales.\n",
        "\n",
        "3. **Population size N=20:** Trade-off between gradient quality (want large N) and computation (want small N). 20 is reasonable for toy problems.\n",
        "\n",
        "4. **Noise scale σ=0.05:** Too large causes divergence, too small causes slow learning. This value was found through trial.\n",
        "\n",
        "5. **Multiple evaluation episodes:** Reduces variance from environment randomness (obstacle placement varies between resets)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Known Limitations\n",
        "\n",
        "1. **State representation:** One-hot encoding doesn't scale beyond ~20×20 grids (400 dimensions)\n",
        "\n",
        "2. **Sample efficiency:** ES requires N×n_episodes evaluations per iteration (100 episodes for N=20, n_episodes=5)\n",
        "\n",
        "3. **Hyperparameter sensitivity:** Performance varies with σ, α, N - no automatic tuning yet\n",
        "\n",
        "4. **No parallelization:** ES perturbations are evaluated sequentially (could parallelize with multiprocessing)\n",
        "\n",
        "5. **Simple policy network:** 2-layer MLP may lack capacity for complex tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Debug/Test Strategies\n",
        "\n",
        "**Debugging ES:**\n",
        "- Print gradient norm (should decrease over time as we converge)\n",
        "- Check if rewards are improving (sanity check)\n",
        "- Visualize policy behavior every N iterations\n",
        "- Test on empty grid first (should quickly reach ~100% success)\n",
        "\n",
        "**Common Issues:**\n",
        "1. Gradient exploding: Reduce σ or α\n",
        "2. No learning: Increase σ or α, check reward function\n",
        "3. High variance: Increase n_eval_episodes or N\n",
        "4. Slow convergence: May need more iterations or different initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Steps\n",
        "\n",
        "1. **Debug environment test failure:** Investigate assertion error in goal detection test\n",
        "2. **Run longer training:** 100+ iterations to verify ES convergence on 8×8 grid\n",
        "3. **Hyperparameter tuning:** Grid search over σ, α, N\n",
        "4. **Implement PPO training loop:** Build `train_ppo` in `src/utils.py` using the existing `ValueNetwork` and `get_action_batch` scaffolding\n",
        "5. **Harder environment:** Test on `HarderGridWorld` (key-door multi-stage task)\n",
        "6. **Parallelization:** Implement multiprocessing for ES evaluations\n",
        "7. **Advanced variants:** Try Natural ES or CMA-ES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a working Evolution Strategies implementation for sparse reward RL:\n",
        "\n",
        "✅ **Problem clearly defined:** Parameter optimization for gridworld navigation  \n",
        "✅ **Mathematics formalized:** ES gradient estimator with explicit update rule  \n",
        "✅ **Implementation complete:** Policy network, ES optimizer, training loop  \n",
        "⚠️ **Validation partial:** ES pipeline runs end-to-end; environment test revealed a bug to investigate; policy has not yet converged in 20 iterations  \n",
        "✅ **Resource monitoring:** ~0.65 seconds/iteration, ~127 MB memory, 8,580 parameters  \n",
        "✅ **Documentation thorough:** Design decisions, limitations, debug strategies  \n",
        "\n",
        "**Current Status:** The ES implementation runs correctly but has not demonstrated convergence on the 8×8 obstacle grid within 20 iterations. Longer training runs (100+ iterations) and hyperparameter tuning are the immediate next steps.\n",
        "\n",
        "See `report.md` for the full project writeup."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
