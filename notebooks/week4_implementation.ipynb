{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 Implementation: Evolution Strategies for Non-Differentiable RL\n",
        "\n",
        "**Date:** February 6, 2026  \n",
        "**Course:** STAT 4830\n",
        "\n",
        "This notebook demonstrates a working implementation comparing Evolution Strategies (ES) with PPO on sparse reward gridworld environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Setup\n",
        "\n",
        "### Clear Problem Statement\n",
        "\n",
        "**Goal:** Learn a policy \u03c0_\u03b8 that navigates from bottom-left to top-right in a gridworld with obstacles.\n",
        "\n",
        "**Challenge:** Rewards are sparse (+1 at goal, 0 elsewhere), making gradient-based learning difficult.\n",
        "\n",
        "**Approach:** Compare parameter-space optimization (ES) vs. action-space RL (PPO)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Formulation\n",
        "\n",
        "**Objective:**\n",
        "$$\\max_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T r_t \\right]$$\n",
        "\n",
        "**Evolution Strategies Gradient Estimate:**\n",
        "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N\\sigma} \\sum_{i=1}^N R(\\theta + \\sigma \\epsilon_i) \\cdot \\epsilon_i$$\n",
        "\n",
        "where $\\epsilon_i \\sim \\mathcal{N}(0, I)$\n",
        "\n",
        "**Update Rule:**\n",
        "$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\nabla_\\theta J(\\theta_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Requirements\n",
        "\n",
        "**Environment:**\n",
        "- State space: 64-dim (8\u00d78 grid, one-hot encoded)\n",
        "- Action space: 4 discrete actions {up, down, left, right}\n",
        "- Episode length: max 50 steps\n",
        "- Obstacles: 8 randomly placed\n",
        "\n",
        "**Training Data:**\n",
        "- Generated online through policy rollouts\n",
        "- ES: 20 perturbations \u00d7 5 episodes = 100 episodes per iteration\n",
        "- PPO: 128 steps per rollout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Success Metrics\n",
        "\n",
        "1. **Success Rate:** % of episodes reaching goal (target: >30%)\n",
        "2. **Average Return:** Mean cumulative reward\n",
        "3. **Learning Stability:** Std dev across trials (lower is better)\n",
        "4. **Sample Efficiency:** Iterations to reach threshold performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# All required imports\n",
        "import sys\n",
        "sys.path.append('../src')  # Add src to path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 4)\n",
        "\n",
        "print(\"Imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imports successful!\n",
            "PyTorch version: 2.8.0\n",
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from model import GridWorld\n",
        "\n",
        "# Test environment\n",
        "env = GridWorld(size=8, n_obstacles=8, max_steps=50, seed=42)\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Environment: {env.size}\u00d7{env.size} grid\")\n",
        "print(f\"State shape: {state.shape}\")\n",
        "print(f\"Action space: {env.n_actions} actions\")\n",
        "print(f\"Start position: {env.start_pos}\")\n",
        "print(f\"Goal position: {env.goal_pos}\")\n",
        "print(f\"Number of obstacles: {len(env.obstacles)}\")\n",
        "\n",
        "# Visualize environment\n",
        "env.render()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment: 8\u00d78 grid\n",
            "State shape: (64,)\n",
            "Action space: 4 actions\n",
            "Start position: (7, 0)\n",
            "Goal position: (0, 7)\n",
            "Number of obstacles: 8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Policy Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from model import PolicyNetwork\n",
        "\n",
        "# Create policy network\n",
        "state_dim = env._get_state().shape[0]  # 64 for 8\u00d78 grid\n",
        "action_dim = env.n_actions  # 4\n",
        "hidden_dim = 64\n",
        "n_layers = 2\n",
        "\n",
        "policy = PolicyNetwork(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    n_layers=n_layers\n",
        ")\n",
        "\n",
        "print(f\"Policy Network:\")\n",
        "print(f\"  Input dim: {state_dim}\")\n",
        "print(f\"  Hidden dim: {hidden_dim}\")\n",
        "print(f\"  Output dim: {action_dim}\")\n",
        "print(f\"  Layers: {n_layers}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in policy.parameters())}\")\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    logits = policy(state_tensor)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    print(f\"\\nTest forward pass:\")\n",
        "    print(f\"  Input shape: {state_tensor.shape}\")\n",
        "    print(f\"  Output shape: {logits.shape}\")\n",
        "    print(f\"  Action probs: {probs.squeeze().numpy()}\")\n",
        "    print(f\"  Sum: {probs.sum().item():.6f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy Network:\n",
            "  Input dim: 64\n",
            "  Hidden dim: 64\n",
            "  Output dim: 4\n",
            "  Layers: 2\n",
            "  Total parameters: 8580\n",
            "\n",
            "Test forward pass:\n",
            "  Input shape: torch.Size([1, 64])\n",
            "  Output shape: torch.Size([1, 4])\n",
            "  Action probs: [0.22053729 0.33520627 0.21455647 0.22969995]\n",
            "  Sum: 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evolution Strategies Implementation\n",
        "\n",
        "ES optimizes by perturbing parameters and estimating gradients from fitness evaluations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_policy(policy, env, n_episodes=5, max_steps=50):\n",
        "    \"\"\"Evaluate policy and return average reward.\"\"\"\n",
        "    total_reward = 0.0\n",
        "    \n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0.0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        \n",
        "        while not done and steps < max_steps:\n",
        "            action, _ = policy.get_action(state, deterministic=False)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "        \n",
        "        total_reward += episode_reward\n",
        "    \n",
        "    return total_reward / n_episodes\n",
        "\n",
        "\n",
        "def es_step(policy, env, N=20, sigma=0.05, n_eval_episodes=5, max_steps=50):\n",
        "    \"\"\"\n",
        "    Single ES optimization step.\n",
        "    \n",
        "    Args:\n",
        "        policy: PolicyNetwork to optimize\n",
        "        env: Environment for evaluation\n",
        "        N: Population size\n",
        "        sigma: Noise scale\n",
        "        n_eval_episodes: Episodes per perturbation\n",
        "        max_steps: Max steps per episode\n",
        "    \n",
        "    Returns:\n",
        "        gradient: Estimated gradient\n",
        "        avg_reward: Average reward across population\n",
        "    \"\"\"\n",
        "    # Get flattened parameters\n",
        "    params = torch.cat([p.flatten() for p in policy.parameters()])\n",
        "    n_params = params.shape[0]\n",
        "    \n",
        "    # Sample perturbations and evaluate\n",
        "    perturbations = []\n",
        "    rewards = []\n",
        "    \n",
        "    for i in range(N):\n",
        "        # Sample perturbation\n",
        "        epsilon = torch.randn(n_params)\n",
        "        perturbations.append(epsilon)\n",
        "        \n",
        "        # Perturb parameters\n",
        "        perturbed_params = params + sigma * epsilon\n",
        "        \n",
        "        # Set perturbed parameters\n",
        "        offset = 0\n",
        "        for p in policy.parameters():\n",
        "            numel = p.numel()\n",
        "            p.data = perturbed_params[offset:offset+numel].view_as(p)\n",
        "            offset += numel\n",
        "        \n",
        "        # Evaluate\n",
        "        reward = evaluate_policy(policy, env, n_eval_episodes, max_steps)\n",
        "        rewards.append(reward)\n",
        "    \n",
        "    # Estimate gradient\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    perturbations = torch.stack(perturbations)\n",
        "    \n",
        "    # Store original average reward before standardization\n",
        "    avg_reward = rewards.mean().item()\n",
        "    \n",
        "    # Standardize rewards for stability (only if there's variance)\n",
        "    if rewards.std() > 1e-8:\n",
        "        rewards_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
        "    else:\n",
        "        # If no variance, use raw rewards (no gradient signal, but at least not NaN)\n",
        "        rewards_normalized = rewards - rewards.mean()\n",
        "    \n",
        "    gradient = (perturbations.T @ rewards_normalized) / (N * sigma)\n",
        "    \n",
        "    # Restore original parameters\n",
        "    offset = 0\n",
        "    for p in policy.parameters():\n",
        "        numel = p.numel()\n",
        "        p.data = params[offset:offset+numel].view_as(p)\n",
        "        offset += numel\n",
        "    \n",
        "    return gradient, avg_reward\n",
        "\n",
        "\n",
        "print(\"ES functions defined successfully!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ES functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ES vs PPO: Fair Comparison (8\u00d78 grid, 8 obstacles, weaker shaping)\n",
        "from model import ValueNetwork\n",
        "from ppo_training import train_ppo\n",
        "\n",
        "# Define shaped reward GridWorld (inherits from GridWorld)\n",
        "class ShapedRewardEnvComparison(GridWorld):\n",
        "    \"\"\"GridWorld with distance-based reward shaping.\"\"\"\n",
        "    def reset(self):\n",
        "        state = super().reset()\n",
        "        self.prev_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
        "        return state\n",
        "    \n",
        "    def step(self, action):\n",
        "        state, reward, done, info = super().step(action)\n",
        "        curr_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
        "        shaped_reward = reward + 0.2 * (self.prev_dist - curr_dist) - 0.01  # Weaker shaping\n",
        "        self.prev_dist = curr_dist\n",
        "        return state, shaped_reward, done, info\n",
        "\n",
        "# Use 8x8 with 8 obstacles (challenging)\n",
        "comparison_env = ShapedRewardEnvComparison(size=8, n_obstacles=8, max_steps=50, seed=123)\n",
        "eval_env = GridWorld(size=8, n_obstacles=8, max_steps=50, seed=123)\n",
        "\n",
        "# 1. Train ES\n",
        "print(\"\\n[1/2] Training ES...\")\n",
        "import time\n",
        "es_start_time = time.time()\n",
        "es_policy = PolicyNetwork(state_dim=64, action_dim=4, hidden_dim=64, n_layers=2)\n",
        "es_params = torch.cat([p.flatten() for p in es_policy.parameters()])\n",
        "\n",
        "for iteration in range(80):\n",
        "    gradient, train_reward = es_step(es_policy, comparison_env, N=50, sigma=0.1, n_eval_episodes=5, max_steps=50)\n",
        "    es_params = es_params + 0.05 * gradient\n",
        "    \n",
        "    offset = 0\n",
        "    for p in es_policy.parameters():\n",
        "        numel = p.numel()\n",
        "        p.data = es_params[offset:offset+numel].view_as(p)\n",
        "        offset += numel\n",
        "    \n",
        "    if iteration % 20 == 0:\n",
        "        print(f\"  ES iter {iteration}/80 - train_reward: {train_reward:.3f}\")\n",
        "\n",
        "# Evaluate ES\n",
        "es_rewards = []\n",
        "es_successes = []\n",
        "for _ in range(20):\n",
        "    state = eval_env.reset()\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < 50:\n",
        "        action, _ = es_policy.get_action(state, deterministic=True)\n",
        "        state, reward, done, info = eval_env.step(action)\n",
        "        ep_reward += reward\n",
        "        steps += 1\n",
        "    es_rewards.append(ep_reward)\n",
        "    es_successes.append(float(info['success']))\n",
        "\n",
        "es_time = time.time() - es_start_time\n",
        "print(f\"\\nES: reward={np.mean(es_rewards):.3f}, success={np.mean(es_successes):.2%}, time={es_time:.1f}s\")\n",
        "\n",
        "# 2. Train PPO\n",
        "print(\"\\n[2/2] Training PPO...\")\n",
        "ppo_start_time = time.time()\n",
        "ppo_policy = PolicyNetwork(state_dim=64, action_dim=4, hidden_dim=64, n_layers=2)\n",
        "ppo_value = ValueNetwork(state_dim=64, hidden_dim=64, n_layers=2)\n",
        "\n",
        "# PPO also trains on shaped rewards (ShapedRewardEnvComparison)\n",
        "trained_ppo_policy, _ = train_ppo(\n",
        "    policy=ppo_policy,\n",
        "    value_net=ppo_value,\n",
        "    env_class=ShapedRewardEnvComparison,\n",
        "    env_kwargs={\"size\": 8, \"n_obstacles\": 8, \"max_steps\": 50, \"seed\": 123},\n",
        "    n_iterations=80,\n",
        "    n_steps=128,\n",
        "    n_epochs=4,\n",
        "    batch_size=64,\n",
        "    lr_policy=3e-4,\n",
        "    lr_value=1e-3,\n",
        "    eval_every=10,\n",
        "    log_wandb=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Evaluate PPO\n",
        "ppo_rewards = []\n",
        "ppo_successes = []\n",
        "for _ in range(20):\n",
        "    state = eval_env.reset()\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < 50:\n",
        "        action, _ = trained_ppo_policy.get_action(state, deterministic=True)\n",
        "        state, reward, done, info = eval_env.step(action)\n",
        "        ep_reward += reward\n",
        "        steps += 1\n",
        "    ppo_rewards.append(ep_reward)\n",
        "    ppo_successes.append(float(info['success']))\n",
        "\n",
        "ppo_time = time.time() - ppo_start_time\n",
        "print(f\"\\nPPO: reward={np.mean(ppo_rewards):.3f}, success={np.mean(ppo_successes):.2%}, time={ppo_time:.1f}s\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Method':<10} {'Avg Reward':<15} {'Success Rate':<15} {'Time':<10}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'ES':<10} {np.mean(es_rewards):>6.3f} \u00b1 {np.std(es_rewards):>5.3f}  {np.mean(es_successes):>6.1%} \u00b1 {np.std(es_successes):>5.1%}  {es_time:>6.1f}s\")\n",
        "print(f\"{'PPO':<10} {np.mean(ppo_rewards):>6.3f} \u00b1 {np.std(ppo_rewards):>5.3f}  {np.mean(ppo_successes):>6.1%} \u00b1 {np.std(ppo_successes):>5.1%}  {ppo_time:>6.1f}s\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "axes[0].boxplot([es_rewards, ppo_rewards], labels=['ES', 'PPO'])\n",
        "axes[0].set_ylabel('Reward')\n",
        "axes[0].set_title('Reward Distribution')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(['ES', 'PPO'], [np.mean(es_successes), np.mean(ppo_successes)], \n",
        "            color=['blue', 'orange'], alpha=0.6)\n",
        "axes[1].set_ylabel('Success Rate')\n",
        "axes[1].set_title('Success Rate Comparison')\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[1/2] Training ES...\n",
            "  ES iter 0/80 - train_reward: -0.499\n",
            "  ES iter 20/80 - train_reward: 2.570\n",
            "  ES iter 40/80 - train_reward: 3.127\n",
            "  ES iter 60/80 - train_reward: 3.568\n",
            "\n",
            "ES: reward=1.000, success=100.00%, time=83.1s\n",
            "\n",
            "[2/2] Training PPO...\n",
            "Iter 1/80: train_reward=-0.200, eval_reward=-0.500, eval_success=0.00, eval_steps=50.0, ema=-0.200, best=-0.200\n",
            "Iter 10/80: train_reward=-0.550, eval_reward=-4.000, eval_success=0.00, eval_steps=50.0, ema=0.084, best=1.950\n",
            "Iter 20/80: train_reward=3.303, eval_reward=2.100, eval_success=0.00, eval_steps=50.0, ema=1.501, best=3.303\n",
            "Iter 30/80: train_reward=1.897, eval_reward=1.900, eval_success=0.00, eval_steps=50.0, ema=2.293, best=3.376\n",
            "Iter 40/80: train_reward=3.147, eval_reward=1.300, eval_success=0.00, eval_steps=50.0, ema=2.763, best=3.428\n",
            "Iter 50/80: train_reward=3.408, eval_reward=1.300, eval_success=0.00, eval_steps=50.0, ema=2.998, best=3.490\n",
            "Iter 60/80: train_reward=3.530, eval_reward=1.900, eval_success=0.00, eval_steps=50.0, ema=3.287, best=3.545\n",
            "Iter 70/80: train_reward=3.418, eval_reward=1.500, eval_success=0.00, eval_steps=50.0, ema=3.389, best=3.585\n",
            "Iter 80/80: train_reward=3.455, eval_reward=3.660, eval_success=1.00, eval_steps=14.0, ema=3.497, best=3.630\n",
            "\n",
            "PPO: reward=1.000, success=100.00%, time=3.2s\n",
            "\n",
            "============================================================\n",
            "COMPARISON SUMMARY\n",
            "============================================================\n",
            "Method     Avg Reward      Success Rate    Time      \n",
            "------------------------------------------------------------\n",
            "ES          1.000 \u00b1 0.000  100.0% \u00b1  0.0%    83.1s\n",
            "PPO         1.000 \u00b1 0.000  100.0% \u00b1  0.0%     3.2s\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/var/folders/xn/81b_v12s5t5317x0pksjtnxr0000gn/T/ipykernel_96612/2845365122.py:131: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Resource Monitoring\n",
        "\n",
        "Track computational resources used by each method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESOURCE MONITORING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "process = psutil.Process(os.getpid())\n",
        "memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "print(f\"\\nMemory: {memory_mb:.1f} MB\")\n",
        "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
        "print(f\"CPU usage: {psutil.cpu_percent(interval=1):.1f}%\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESOURCE MONITORING\n",
            "============================================================\n",
            "\n",
            "Memory: 110.3 MB\n",
            "CPU cores: 8\n",
            "CPU usage: 45.7%\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}